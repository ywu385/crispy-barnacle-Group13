{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "978b7936",
   "metadata": {},
   "source": [
    "# CS109A:  Project Milestone 2 - Group 13\n",
    "\n",
    "### Project 5 - Housing Crisis & Homelessness\n",
    "\n",
    "Group 13: Andrew Russo, John Wu, Ryan Han, Vivian Lu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7d0997bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data_path = os.path.join(os.getcwd(),'Data')\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7c6042",
   "metadata": {},
   "source": [
    "<hr style=\"height:2pt\">\n",
    "\n",
    "### Current Question: \n",
    "\n",
    "By recent estimates, over 600,000 Americans live in some state of homelessness each year. In recent decades, the quality of available data on homelessness in the United States has greatly improved, but there is still a long way to go. In this project you will work to visualize trends and create models to understand the top predictors of homelessness. Some questions you may persue are: \n",
    "\n",
    "**Have the top predictors changed over time? What events or policies took place that led to a change in the intensity of homelessness (either positive or negative)?** \n",
    "\n",
    "Data at the national level should be incorporated in some way even if you decided to focus on a specific city such as LA, NYC, or Boston. \n",
    "\n",
    "### Current Data Sources:\n",
    "\n",
    "https://www.kaggle.com/schirmerchad/bostonhoustingmlnd <br/>\n",
    "https://www.kaggle.com/adamschroeder/homelessness-comparison-between-states/data <br/>\n",
    "https://www.kaggle.com/umerkk12/homelessness-in-us\n",
    "\n",
    "### Additional Data Sources (Per Vivian):\n",
    "https://www.hudexchange.info/resource/3031/pit-and-hic-data-since-2007/\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990c2c08",
   "metadata": {},
   "source": [
    "Description of the Data:\n",
    "1. What type of data are you dealing with?\n",
    "2. What methods have you used to explore the data (initial explorations, data cleaning, etc)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49091b7",
   "metadata": {},
   "source": [
    "Abbreviations:\n",
    "- HIC = Housing Inventory Count\n",
    "- PIT = Point in Time\n",
    "- ES = Emergency Shelter \n",
    "- TH = Transitional Housing \n",
    "- SH = Safe Haven\n",
    "- RRH = Rapid Re-Housing\n",
    "- PSH = Permanent Supportive Housing\n",
    "- OPH = Other Permenanent Housing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "875d6ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to help with cleaning dataframes\n",
    "\n",
    "class worksheets:\n",
    "    \n",
    "    # lists of features to parse\n",
    "    \n",
    "    race_list = ['White','Black or African American',\n",
    "                 'American Indian or Alaska Native','Multiple Races',\n",
    "                 'Asian','Native Hawaiian or Other Pacific Islander']\n",
    "    eth_list = ['Hispanic/Latino','Non-Hispanic/Non-Latino']\n",
    "    gender_list = ['Transgender','Gender Non-Conforming','Male','Female']\n",
    "    age_list = ['Under 18','Age 18 to 24','Over 24']\n",
    "    homeless_type = ['ES','SH','TH','RRH','PSH','OPH']\n",
    "    group_type =['Individuals','Families','Unaccompanied Youth (Under 25)','Children of Parenting Youth',\n",
    "                 'Parenting Youth Under 18','Parenting Youth Age 18-24']\n",
    "    count_list = ['Overall','Unsheltered','Sheltered']\n",
    "\n",
    "\n",
    "    def __init__(self,data, name):\n",
    "        self.data = data\n",
    "        self.name = name\n",
    "        self.feature_lists = [race_list, eth_list,\n",
    "                              gender_list, age_list,\n",
    "                              homeless_type, group_type,\n",
    "                              count_list]\n",
    "        self.feature_names = ['Race','Ethnicity',\n",
    "                              'Gender','Age_Group',\n",
    "                              'Shelter_Type','Group_Type',\n",
    "                              'Count_Type']\n",
    "\n",
    "    def feat_parser(self, var, l_values, upper=True):\n",
    "         if upper == True:\n",
    "            var = var.upper()\n",
    "            for feature in l_values:\n",
    "                feature = feature.upper()\n",
    "                if feature in var:\n",
    "                    return feature\n",
    "            return None\n",
    "         else:\n",
    "            for feature in l_values:\n",
    "                if feature in var:\n",
    "                    return feature\n",
    "            return None\n",
    "    \n",
    "    def to_feature(self):\n",
    "        data = self.data\n",
    "        for feature, feat_list in zip(self.feature_names, self.feature_lists):\n",
    "            if feature == 'Shelter_Type':\n",
    "                upper = False\n",
    "            data[feature] = [self.feat_parser(var,feat_list) for var in data['variable']]\n",
    "        self.data = data\n",
    "        \n",
    "        \n",
    "# helper function to read in data\n",
    "\n",
    "def read_data(data_file, type_of_table):\n",
    "    if type_of_table == 'ST':\n",
    "        index_col = ['State','Number of CoCs', 'Year']\n",
    "        total_col = 'State'\n",
    "        skipfooter = 1\n",
    "    else:\n",
    "        index_col = ['CoC Number','CoC Name','Year']\n",
    "        total_col ='CoC Name'\n",
    "    wb = pd.ExcelFile(data_file)\n",
    "    sheets = wb.sheet_names\n",
    "    df = []\n",
    "    for sheet in sheets:\n",
    "           if sheet != 'Revisions' and sheet != 'CoC Mergers':\n",
    "            #print(sheet)  # Used for debugging\n",
    "            df_sheet = pd.read_excel(data_file, sheet_name = sheet)\n",
    "            df_sheet['Year'] = sheet\n",
    "\n",
    "            df_sheet = df_sheet.drop(df_sheet.filter(regex='Total').columns, axis=1) # dropping total columns since these would double count observations\n",
    "            df_sheet = df_sheet[df_sheet[total_col]!= 'Total'] #dropping total \n",
    "\n",
    "            keep_cols = set(index_col)\n",
    "            melt_cols = list(set(list(df_sheet.columns)) - keep_cols)\n",
    "\n",
    "            df_sheet = pd.melt(df_sheet, id_vars=index_col,value_vars = melt_cols)\n",
    "            df.append(df_sheet)\n",
    "    return pd.concat(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0d3a1770",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data_files\n",
    "PIT_ST_file = data_path+'/2007-2021-PIT-Counts-by-State.xlsx'\n",
    "PIT_CoC_file = data_path+'/2007-2021-PIT-Counts-by-CoC.xlsx'\n",
    "\n",
    "\n",
    "# reading files\n",
    "PIT_ST = read_data(PIT_ST_file,'ST')\n",
    "PIT_CoC = read_data(PIT_CoC_file,'CoC')\n",
    "PIT_CoC.dropna(subset='CoC Name',inplace=True) # dropping extra footers and totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a9c99dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding features\n",
    "\n",
    "PIT_ST = worksheets(PIT_ST,'ST')\n",
    "PIT_ST.to_feature()\n",
    "\n",
    "PIT_CoC = worksheets(PIT_CoC, 'CoC')\n",
    "PIT_CoC.to_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "9d23cefe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This sheet is too large! Your sheet size is: 1139039, 12 Max sheet size is: 1048576, 16384",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [182], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m PIT_ST_df\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPIT_ST_Cleaned.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m PIT_CoC_df \u001b[38;5;241m=\u001b[39m PIT_CoC\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m----> 6\u001b[0m \u001b[43mPIT_CoC_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPIT_CoC_Cleaned.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/cs109a/lib/python3.10/site-packages/pandas/core/generic.py:2345\u001b[0m, in \u001b[0;36mNDFrame.to_excel\u001b[0;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, encoding, inf_rep, verbose, freeze_panes, storage_options)\u001b[0m\n\u001b[1;32m   2332\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexcel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[1;32m   2334\u001b[0m formatter \u001b[38;5;241m=\u001b[39m ExcelFormatter(\n\u001b[1;32m   2335\u001b[0m     df,\n\u001b[1;32m   2336\u001b[0m     na_rep\u001b[38;5;241m=\u001b[39mna_rep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2343\u001b[0m     inf_rep\u001b[38;5;241m=\u001b[39minf_rep,\n\u001b[1;32m   2344\u001b[0m )\n\u001b[0;32m-> 2345\u001b[0m \u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2347\u001b[0m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2353\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/cs109a/lib/python3.10/site-packages/pandas/io/formats/excel.py:877\u001b[0m, in \u001b[0;36mExcelFormatter.write\u001b[0;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options)\u001b[0m\n\u001b[1;32m    875\u001b[0m num_rows, num_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_rows \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_rows \u001b[38;5;129;01mor\u001b[39;00m num_cols \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_cols:\n\u001b[0;32m--> 877\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    878\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis sheet is too large! Your sheet size is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax sheet size is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    880\u001b[0m     )\n\u001b[1;32m    882\u001b[0m formatted_cells \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_formatted_cells()\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(writer, ExcelWriter):\n",
      "\u001b[0;31mValueError\u001b[0m: This sheet is too large! Your sheet size is: 1139039, 12 Max sheet size is: 1048576, 16384"
     ]
    }
   ],
   "source": [
    "# outputting data\n",
    "PIT_ST_df = PIT_ST.data\n",
    "PIT_ST_df.to_excel('PIT_ST_Cleaned.xlsx')\n",
    "\n",
    "PIT_CoC_df = PIT_CoC.data\n",
    "PIT_CoC_df.to_excel('PIT_CoC_Cleaned.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555a6e26",
   "metadata": {},
   "source": [
    "Visualizations and Captions that summarize the noteworthy findings of the exploratory data analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b2f447da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f8b9d3",
   "metadata": {},
   "source": [
    "Revised project question based on your group's interests and the insights gained through EDA.  Be sure to keep the scope manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0d5a1c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f7da3d",
   "metadata": {},
   "source": [
    "A baseline mode.  (implemented or clearly described)\n",
    "\n",
    "* baseline model should be more sophisticated than a \"naive\" model like in HW5 that simply predicts the majority class or mean response value.  Through class proportions (if dealing with a classification problem) is importnat information to repore in your EDA as this will almost certainly inform later modeling choices. \n",
    "\n",
    "(HW5 Naive model = if we simply apply the same outcome to all applicants based on the proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a36390e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93371654",
   "metadata": {},
   "source": [
    "2 or more references (papers, articles, etc.) that your groups thinks will be relevant to guiding your project work. These can be related to methods and/or the problem domain of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef0e3c0",
   "metadata": {},
   "source": [
    "**John**:\n",
    "- National Alliance to end homelessness (endhomelessness.org)\n",
    "   - Provides additional dataset between 2007 to 2021.  (COVID data from 2021 to 2022 is more incomplete)\n",
    "   - Adds HUD dataset.  \n",
    "   \n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
